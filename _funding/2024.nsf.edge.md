---
title: "SHF: Provably Correct, Efficient-Efficient Edge Computing"
source: nsf
tag: 2024.nsf.edge
date: 2024-07-01
projects:
    sonic
    dataflow
---

As technology scaling tapers off, CPUs can no longer provide the efficiency gains (in TOPS/W) that computing has historically achieved. Fundamental design choices baked into von Neumann processors inherently limit their efficiency, creating an urgent need to rethink general-purpose computation.

Current von Neumann processors are over-engineered for general-purpose operation. Every instruction can potentially read/write any register and, in many architectures, any memory address — but real programs have much more structure than this. A CPU’s needlessly generalized interface requires large, energy-hungry hardware structures (register files, issue queues, re-order buffers). The complexity introduced in these designs complicates compilers, increases development time, invites bugs and security flaws, and impedes verifiability. Moreover, von Neumann computing is fundamentally sequential. The program counter (plus control operations) creates a linear sequence of instructions to execute, while memory operations must take effect in load-store order. Modern superscalar designs go to heroic lengths to dynamically discover parallelism as they execute code, but this increases complexity and energy usage.

To improve efficiency, architects have turned to specialized hardware accelerators (ASICs), often including dozens in a given system-on-chip. However, designing a custom ASIC involves a large and risky investment of time, money, and personnel with specialized skillsets. Worse, ASICs abandon programmability, and hence assume that a particular computation will remain relevant, will be used in the same manner, and will not be superseded by other advances in the field. These considerations limit the computations for which ASICs are useful or, conversely, make ASICs a barrier to innovation because, once developed, they impose a large efficiency tax on any computation that is not ASIC-supported.

This proposal’s grand challenge is to redefine general-purpose computer architectures to provide the performance and efficiency of ASICs without their inflexibility, to retain the programmability of von Neumann cores without their inherent overheads, and to expose simple interfaces and microarchitectures that can be fully verified for correctness and security.

We will address this challenge by abandoning the complexity and inherent sequentiality of von Neumann processors and designing a new class of hierarchical, reconfigurable dataflow architectures, unlocking verifiability and scalability unavailable to existing architectures. Our ultimate goal is to produce a family of fully programmable, provably correct (and secure) computer systems that outpace current designs in energy efficiency and performance by at least 10×.

Achieving this goal will require new breakthroughs in the hierarchical design of dataflow processors and redefining the software-hardware contract around a spatial ABI. Furthermore, a radical revamp to the computing architecture gives us an opportunity to build in correctness and security from the beginning, enabled by the simplicity and modularity of dataflow architectures.

Our open source results will will open new frontiers of computing and pave a trail for the industry to dramatically increase available computational power, facilitated by the PIs’ extensive ties with industry partners. At the low-power end of the spectrum, the power-savings and programmability offered by dataflow computing will mean that sensors can be deployed for years (rather than weeks) in the field. At datacenter scale, dataflow computing will enable another leap forward in computational power without a commensurate increase in energy usage. 